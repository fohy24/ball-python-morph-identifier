{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, utils, models\n",
    "# from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import altair as alt\n",
    "alt.data_transformers.enable(\"vegafusion\")\n",
    "\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change csv name as needed\n",
    "labels_extended = pd.read_csv('data/selected_gene_df.csv')\n",
    "clean_possible_genes = labels_extended.columns.to_list()[7:]\n",
    "num_labels = len(clean_possible_genes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of trainset: (41416, 37)\n",
      "Shape of testset: (7309, 37)\n"
     ]
    }
   ],
   "source": [
    "train_df, test_df = train_test_split(labels_extended, train_size=0.85, random_state=123)\n",
    "# test_df.set_index('index', inplace=True)\n",
    "print(f'Shape of trainset: {train_df.shape}')\n",
    "print(f'Shape of testset: {test_df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonGeneDataset(Dataset):\n",
    "    def __init__(self, labels_df, img_dir, indices=None, transform=None):\n",
    "        self.labels_df = labels_df\n",
    "        if indices is not None:\n",
    "            self.labels_df = self.labels_df.iloc[indices]\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.img_dir, f\"{self.labels_df.iloc[idx, 0]}.png\")\n",
    "        image = Image.open(img_name)\n",
    "        # Parse labels here based on your CSV structure and required format\n",
    "        labels = torch.tensor(self.labels_df.iloc[idx, 7:].astype('float32').values)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/deep-learning/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = 480\n",
    "\n",
    "new_layers = nn.Sequential(\n",
    "    nn.LazyLinear(2048),  \n",
    "    nn.BatchNorm1d(2048),\n",
    "    nn.ReLU(),             \n",
    "    nn.Dropout(0.5),       \n",
    "    nn.LazyLinear(num_labels)\n",
    ")\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "    v2.ToDtype(torch.float32),\n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "# If using GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "full_dataset = PythonGeneDataset(labels_df=train_df, img_dir='data/img/', transform=transform)\n",
    "\n",
    "# Split dataset\n",
    "total_size = len(full_dataset)\n",
    "train_size = int(0.8 * total_size)\n",
    "valid_size = total_size - train_size\n",
    "generator1 = torch.Generator(device='cpu').manual_seed(42)\n",
    "train_indices, valid_indices = torch.utils.data.random_split(\n",
    "    np.arange(total_size),\n",
    "    [train_size, valid_size],\n",
    "    generator=generator1\n",
    "    )\n",
    "train_indices, valid_indices = torch.utils.data.random_split(np.arange(total_size), [train_size, valid_size])\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_dataset = Subset(full_dataset, train_indices)\n",
    "valid_dataset = Subset(full_dataset, valid_indices)\n",
    "\n",
    "# Initialize DataLoaders\n",
    "BATCH_SIZE = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=multiprocessing.cpu_count())\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003070, 0.006001\n"
     ]
    }
   ],
   "source": [
    "efficientnet = models.efficientnet_v2_l(weights='EfficientNet_V2_L_Weights.DEFAULT')\n",
    "efficientnet.classifier = new_layers\n",
    "\n",
    "checkpoint = torch.load(f'model/model_v13_1_epoch9.pt', map_location=device)\n",
    "efficientnet.load_state_dict(checkpoint['model_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "train_loss = checkpoint['train_loss']\n",
    "valid_loss = checkpoint['valid_loss']\n",
    "\n",
    "efficientnet.eval()\n",
    "\n",
    "efficientnet.to(device)\n",
    "print(f\"{train_loss:.6f}, {valid_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Prediction from Test set\n",
    "# img_code = np.random.choice(test_df.index)\n",
    "# img = Image.open(f'data/img/{img_code}-0.png')\n",
    "# input_img = transform(img)\n",
    "# input_img = input_img.unsqueeze(0)\n",
    "# input_img = input_img.to(device)\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     output = efficientnet(input_img)\n",
    "\n",
    "# predicted_probs = torch.sigmoid(output).to('cpu')\n",
    "# prediction = pd.DataFrame(predicted_probs, index=['predictions'],\n",
    "#                           columns=clean_possible_genes).T.sort_values(by=['predictions'], ascending=False)\n",
    "\n",
    "# print(f'True Morphs: {test_df.loc[img_code, \"genes\"]}')\n",
    "# display(prediction.query('predictions > 0.5'))\n",
    "# img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for all the pictures for a listing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work because of random train test split\n",
    "\n",
    "# img_code = np.random.choice(test_df.index)\n",
    "# img_code_parent = img_code.split(\"-\")[0]\n",
    "# test_df.index[test_df.index.astype(str).str.contains(fr'^{img_code_parent}-')].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2.0, alpha=0.25, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduction='none')\n",
    "        pt = torch.exp(-BCE_loss) \n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        \n",
    "        if self.reduction == 'mean':\n",
    "            return torch.mean(F_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return torch.sum(F_loss)\n",
    "        else:\n",
    "            return F_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on Test set: 0.00574\n"
     ]
    }
   ],
   "source": [
    "# Loss on test set\n",
    "test_dataset = PythonGeneDataset(labels_df=test_df, img_dir='data/img/', transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "focal_loss = FocalLoss()\n",
    "criterion = focal_loss\n",
    "\n",
    "efficientnet.eval()  # Set model to evaluate mode\n",
    "\n",
    "test_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = efficientnet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "# Calculate average loss over validation data\n",
    "test_loss = test_loss / len(test_loader.dataset)\n",
    "print(f'Loss on Test set: {test_loss:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the whole test set\n",
    "test_df_pred = {}\n",
    "\n",
    "for img_code in test_df[\"index\"]:\n",
    "    img = Image.open(f'data/img/{img_code}.png')\n",
    "    input_img = transform(img)\n",
    "    input_img = input_img.unsqueeze(0)\n",
    "    input_img = input_img.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = efficientnet(input_img)\n",
    "\n",
    "    predicted_probs = torch.sigmoid(output).to('cpu')\n",
    "    test_df_pred[img_code] = predicted_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = {key: val.numpy().flatten() for key, val in test_df_pred.items()}\n",
    "pred_df = pd.DataFrame.from_dict(pred_df, orient='index', columns=clean_possible_genes)\n",
    "pred_df_round = pred_df.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_matching_elements(list1, list2):\n",
    "    count = sum(1 for x, y in zip(list1, list2) if x == y)\n",
    "    return count\n",
    "\n",
    "count_matching_elements(test_df.query(\"index == '27721-0'\").iloc[0,7:].to_list(), pred_df_round.loc[\"27721-0\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction with 0 mistake: 0.538\n",
      "Prediction with 1 mistake: 0.265\n",
      "Prediction with 2 mistake: 0.131\n",
      "Prediction with 3 mistake: 0.046\n",
      "Prediction with 4 mistake: 0.014\n",
      "Prediction with 5 mistake: 0.004\n",
      "Prediction with 6 mistake: 0.001\n",
      "Prediction with 7 mistake: 0.001\n",
      "Prediction with 8 mistake: 0.000\n",
      "Prediction with 9 mistake: 0.000\n",
      "Prediction with 10 mistake: 0.000\n",
      "Prediction with 11 mistake: 0.000\n",
      "Prediction with 12 mistake: 0.000\n",
      "Prediction with 13 mistake: 0.000\n",
      "Prediction with 14 mistake: 0.000\n",
      "Prediction with 15 mistake: 0.000\n",
      "Prediction with 16 mistake: 0.000\n",
      "Prediction with 17 mistake: 0.000\n",
      "Prediction with 18 mistake: 0.000\n",
      "Prediction with 19 mistake: 0.000\n",
      "Prediction with 20 mistake: 0.000\n",
      "Prediction with 21 mistake: 0.000\n",
      "Prediction with 22 mistake: 0.000\n",
      "Prediction with 23 mistake: 0.000\n",
      "Prediction with 24 mistake: 0.000\n",
      "Prediction with 25 mistake: 0.000\n",
      "Prediction with 26 mistake: 0.000\n",
      "Prediction with 27 mistake: 0.000\n",
      "Prediction with 28 mistake: 0.000\n",
      "Prediction with 29 mistake: 0.000\n",
      "Prediction with 30 mistake: 0.000\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for idx in pred_df_round.index:\n",
    "    pred = count_matching_elements(test_df.query(f\"index == '{idx}'\").iloc[0,7:].to_list(), pred_df_round.loc[idx].to_list())\n",
    "    result.append(pred)\n",
    "    \n",
    "for n in range(30, -1, -1):\n",
    "    print(f'Prediction with {30-n} mistake: {result.count(n) / len(result):.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
